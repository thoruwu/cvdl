{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-13T17:27:13.887007Z",
     "iopub.status.busy": "2024-11-13T17:27:13.886261Z",
     "iopub.status.idle": "2024-11-13T17:27:17.176345Z",
     "shell.execute_reply": "2024-11-13T17:27:17.175562Z",
     "shell.execute_reply.started": "2024-11-13T17:27:13.886963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T17:27:19.474530Z",
     "iopub.status.busy": "2024-11-13T17:27:19.473474Z",
     "iopub.status.idle": "2024-11-13T17:27:19.645115Z",
     "shell.execute_reply": "2024-11-13T17:27:19.644142Z",
     "shell.execute_reply.started": "2024-11-13T17:27:19.474470Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK data for tokenization\n",
    "nltk.download('punkt')\n",
    "if torch.cuda.is_available():\n",
    "    device=torch.device(type=\"cuda\",index=0)\n",
    "else:\n",
    "    device=torch.device(type=\"cpu\",index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T17:27:21.335083Z",
     "iopub.status.busy": "2024-11-13T17:27:21.334184Z",
     "iopub.status.idle": "2024-11-13T17:27:21.340258Z",
     "shell.execute_reply": "2024-11-13T17:27:21.339229Z",
     "shell.execute_reply.started": "2024-11-13T17:27:21.335040Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "image_size = 224\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "freq_threshold = 5\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T17:27:22.113949Z",
     "iopub.status.busy": "2024-11-13T17:27:22.113538Z",
     "iopub.status.idle": "2024-11-13T17:27:22.119458Z",
     "shell.execute_reply": "2024-11-13T17:27:22.118378Z",
     "shell.execute_reply.started": "2024-11-13T17:27:22.113910Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T17:27:25.065459Z",
     "iopub.status.busy": "2024-11-13T17:27:25.065048Z",
     "iopub.status.idle": "2024-11-13T17:27:25.074776Z",
     "shell.execute_reply": "2024-11-13T17:27:25.073753Z",
     "shell.execute_reply.started": "2024-11-13T17:27:25.065420Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Vocabulary class to build word-to-index and index-to-word mappings\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4  # Starting index for new words\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            tokens = word_tokenize(sentence.lower())\n",
    "            frequencies.update(tokens)\n",
    "\n",
    "            for token, freq in frequencies.items():\n",
    "                if freq >= self.freq_threshold and token not in self.stoi:\n",
    "                    self.stoi[token] = idx\n",
    "                    self.itos[idx] = token\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T17:27:26.209160Z",
     "iopub.status.busy": "2024-11-13T17:27:26.208736Z",
     "iopub.status.idle": "2024-11-13T17:28:06.282724Z",
     "shell.execute_reply": "2024-11-13T17:28:06.281697Z",
     "shell.execute_reply.started": "2024-11-13T17:27:26.209113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Building the vocabulary\n",
    "captions_file = '/kaggle/input/flickr-8k-images/Flickr8k/Flickr8k_text/Flickr8k.token.txt'\n",
    "img_folder = '/kaggle/input/flickr-8k-images/Flickr8k/Flickr8k_Dataset/Flicker8k_Dataset'\n",
    "captions_list = []\n",
    "\n",
    "# Read all captions and build vocabulary\n",
    "with open(captions_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        arr = line.strip().split('\\t')\n",
    "        if len(arr) > 1:\n",
    "            captions_list.append(arr[1])\n",
    "\n",
    "vocab = Vocabulary(freq_threshold)\n",
    "vocab.build_vocabulary(captions_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T17:28:17.534480Z",
     "iopub.status.busy": "2024-11-13T17:28:17.534064Z",
     "iopub.status.idle": "2024-11-13T17:28:17.541238Z",
     "shell.execute_reply": "2024-11-13T17:28:17.540108Z",
     "shell.execute_reply.started": "2024-11-13T17:28:17.534441Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting token.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile token.txt\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T17:28:18.461957Z",
     "iopub.status.busy": "2024-11-13T17:28:18.461566Z",
     "iopub.status.idle": "2024-11-13T17:28:24.420456Z",
     "shell.execute_reply": "2024-11-13T17:28:24.419476Z",
     "shell.execute_reply.started": "2024-11-13T17:28:18.461919Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image not found: 2258277193_586949ec62.jpg.1. Skipping this entry.\n",
      "Image not found: 2258277193_586949ec62.jpg.1. Skipping this entry.\n",
      "Image not found: 2258277193_586949ec62.jpg.1. Skipping this entry.\n",
      "Image not found: 2258277193_586949ec62.jpg.1. Skipping this entry.\n",
      "Image not found: 2258277193_586949ec62.jpg.1. Skipping this entry.\n",
      "Updated captions file saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def preprocess_images(captions_file, img_folder):\n",
    "    \"\"\"\n",
    "    Preprocess the captions and image paths.\n",
    "    Checks if each image in the captions file exists in the img_folder.\n",
    "    If an image does not exist, it is removed from the captions file.\n",
    "    \n",
    "    Args:\n",
    "        captions_file (str): Path to the file containing captions.\n",
    "        img_folder (str): Path to the folder containing images.\n",
    "        \n",
    "    Returns:\n",
    "        None: Updates the captions file in place with only existing images.\n",
    "    \"\"\"\n",
    "    filtered_captions = []\n",
    "\n",
    "    # Load captions and check for image existence\n",
    "    with open(captions_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            img_id, caption = line.strip().split('\\t')\n",
    "            img_name = img_id.split('#')[0]  # Strip out caption ID if present\n",
    "            img_path = os.path.join(img_folder, img_name)\n",
    "            \n",
    "            # Check if image exists in the folder\n",
    "            if os.path.exists(img_path):\n",
    "                filtered_captions.append(f\"{img_id}\\t{caption}\")\n",
    "            else:\n",
    "                print(f\"Image not found: {img_name}. Skipping this entry.\")\n",
    "    \n",
    "    # Write the filtered captions back to the original file\n",
    "    with open('/kaggle/working/token.txt', 'w') as file:\n",
    "        file.write('\\n'.join(filtered_captions))\n",
    "    print(\"Updated captions file saved.\")\n",
    "\n",
    "# Paths\n",
    "# captions_file = '/content/drive/MyDrive/Image_caption/Flickr8k_text/Flickr8k_text/Flickr8k.token.txt'\n",
    "# img_folder = '/content/drive/MyDrive/Image_caption/Flicker8k_Dataset/Flicker8k_Dataset'\n",
    "\n",
    "# Run the preprocessing function to update the captions file\n",
    "preprocess_images(captions_file, img_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T17:28:34.543254Z",
     "iopub.status.busy": "2024-11-13T17:28:34.542366Z",
     "iopub.status.idle": "2024-11-13T17:28:34.547203Z",
     "shell.execute_reply": "2024-11-13T17:28:34.546128Z",
     "shell.execute_reply.started": "2024-11-13T17:28:34.543211Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "captions_file='/kaggle/working/token.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T17:28:35.654646Z",
     "iopub.status.busy": "2024-11-13T17:28:35.653877Z",
     "iopub.status.idle": "2024-11-13T17:28:35.659742Z",
     "shell.execute_reply": "2024-11-13T17:28:35.658758Z",
     "shell.execute_reply.started": "2024-11-13T17:28:35.654601Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3005\n",
      "A child in a pink dress is climbing up a set of stairs in an entry way .\n"
     ]
    }
   ],
   "source": [
    "print(vocab.__len__())\n",
    "print(captions_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T17:28:36.705566Z",
     "iopub.status.busy": "2024-11-13T17:28:36.705161Z",
     "iopub.status.idle": "2024-11-13T17:28:36.718670Z",
     "shell.execute_reply": "2024-11-13T17:28:36.717718Z",
     "shell.execute_reply.started": "2024-11-13T17:28:36.705527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, img_folder, captions_file, transform=None, vocab=None):\n",
    "        self.img_folder = img_folder\n",
    "        self.transform = transform\n",
    "        self.vocab = vocab\n",
    "        self.captions = self.load_captions(captions_file)\n",
    "\n",
    "    def load_captions(self, captions_file):\n",
    "        with open(captions_file, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "        captions = {}\n",
    "        for line in lines:\n",
    "            img, caption = line.strip().split('\\t')\n",
    "            img_id = img.split('#')[0]\n",
    "            if img_id not in captions:\n",
    "                captions[img_id] = []\n",
    "            captions[img_id].append(caption)\n",
    "        return captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = list(self.captions.keys())[idx]\n",
    "        img_path = os.path.join(self.img_folder, img_id)\n",
    "        \n",
    "        # Verify if the image exists\n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"Warning: File {img_path} does not exist.\")\n",
    "            raise IndexError  # Skip this item in the DataLoader\n",
    "\n",
    "        # Load image and apply transformations\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Process caption\n",
    "        caption = self.captions[img_id][0]\n",
    "        caption = [self.vocab.stoi[\"<SOS>\"]] + self.vocab.numericalize(caption) + [self.vocab.stoi[\"<EOS>\"]]\n",
    "        \n",
    "        return image, caption  # Return caption as a list, not a tensor\n",
    "\n",
    "# Updated DataLoader with error handling\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b is not None]  # Remove None entries if any images were skipped\n",
    "    images = []\n",
    "    captions = []\n",
    "\n",
    "    for img, caption in batch:\n",
    "        images.append(img)\n",
    "        captions.append(torch.tensor(caption, dtype=torch.long))\n",
    "\n",
    "    # Stack images and pad captions\n",
    "    images = torch.stack(images)\n",
    "    captions = pad_sequence(captions, batch_first=True, padding_value=vocab.stoi[\"<PAD>\"])\n",
    "    return images, captions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T17:28:37.666223Z",
     "iopub.status.busy": "2024-11-13T17:28:37.665822Z",
     "iopub.status.idle": "2024-11-13T17:28:38.472409Z",
     "shell.execute_reply": "2024-11-13T17:28:38.471610Z",
     "shell.execute_reply.started": "2024-11-13T17:28:37.666184Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Encoder model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.bn(self.fc(features))\n",
    "        return features\n",
    "\n",
    "# Decoder model\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "dataset = Flickr8kDataset(img_folder, captions_file, transform=transform, vocab=vocab)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize models, loss, and optimizer\n",
    "encoder = Encoder(embed_size=embedding_dim).to(device)\n",
    "decoder = Decoder(embed_size=embedding_dim, hidden_size=hidden_dim, vocab_size=len(vocab)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])\n",
    "params = list(decoder.parameters()) + list(encoder.fc.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = optim.Adam(params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T17:28:38.846747Z",
     "iopub.status.busy": "2024-11-13T17:28:38.845939Z",
     "iopub.status.idle": "2024-11-13T17:41:51.403462Z",
     "shell.execute_reply": "2024-11-13T17:41:51.402385Z",
     "shell.execute_reply.started": "2024-11-13T17:28:38.846698Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10], Step [0/253], Loss: 8.0075\n",
      "Epoch [0/10], Step [100/253], Loss: 3.5962\n",
      "Epoch [0/10], Step [200/253], Loss: 3.1139\n",
      "Epoch [1/10], Step [0/253], Loss: 3.1001\n",
      "Epoch [1/10], Step [100/253], Loss: 2.9630\n",
      "Epoch [1/10], Step [200/253], Loss: 2.8545\n",
      "Epoch [2/10], Step [0/253], Loss: 2.3913\n",
      "Epoch [2/10], Step [100/253], Loss: 2.7825\n",
      "Epoch [2/10], Step [200/253], Loss: 2.4289\n",
      "Epoch [3/10], Step [0/253], Loss: 2.3332\n",
      "Epoch [3/10], Step [100/253], Loss: 2.1646\n",
      "Epoch [3/10], Step [200/253], Loss: 2.8367\n",
      "Epoch [4/10], Step [0/253], Loss: 2.1732\n",
      "Epoch [4/10], Step [100/253], Loss: 2.2393\n",
      "Epoch [4/10], Step [200/253], Loss: 2.2535\n",
      "Epoch [5/10], Step [0/253], Loss: 1.9444\n",
      "Epoch [5/10], Step [100/253], Loss: 2.0729\n",
      "Epoch [5/10], Step [200/253], Loss: 1.8761\n",
      "Epoch [6/10], Step [0/253], Loss: 1.7802\n",
      "Epoch [6/10], Step [100/253], Loss: 1.6981\n",
      "Epoch [6/10], Step [200/253], Loss: 1.8455\n",
      "Epoch [7/10], Step [0/253], Loss: 1.5268\n",
      "Epoch [7/10], Step [100/253], Loss: 1.6378\n",
      "Epoch [7/10], Step [200/253], Loss: 1.6427\n",
      "Epoch [8/10], Step [0/253], Loss: 1.3428\n",
      "Epoch [8/10], Step [100/253], Loss: 1.4355\n",
      "Epoch [8/10], Step [200/253], Loss: 1.5232\n",
      "Epoch [9/10], Step [0/253], Loss: 1.2151\n",
      "Epoch [9/10], Step [100/253], Loss: 1.2122\n",
      "Epoch [9/10], Step [200/253], Loss: 1.3504\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, captions) in enumerate(data_loader):\n",
    "        images, captions = images.to(device), captions.to(device)\n",
    "\n",
    "        # Forward pass through encoder\n",
    "        features = encoder(images)\n",
    "\n",
    "        # Pass all tokens except the last one to the decoder\n",
    "        outputs = decoder(features, captions[:, :-1])  # Predict the next token for each token in captions\n",
    "\n",
    "\n",
    "        outputs = outputs.view(-1, outputs.shape[2])  # Flatten to (batch_size * (seq_len - 1), vocab_size)\n",
    "        targets = captions[:, :].contiguous().view(-1)  # Flatten to (batch_size * (seq_len - 1))\n",
    "\n",
    "        # Debug: Print shapes to verify alignment before loss calculation\n",
    "        # print(f\"Outputs shape: {outputs.shape}\")  # Should be (batch_size * (seq_len - 1), vocab_size)\n",
    "        # print(f\"Targets shape: {targets.shape}\")  # Should be (batch_size * (seq_len - 1))\n",
    "\n",
    "        # Calculate loss, ignoring <PAD> tokens\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Step [{i}/{len(data_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def load_image(image_path, transform):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image.to(device)\n",
    "\n",
    "def generate_caption(encoder, decoder, image_tensor, vocab, max_length=20):\n",
    "    # Get image features from encoder\n",
    "    with torch.no_grad():\n",
    "        features = encoder(image_tensor).unsqueeze(1)  # Shape: (1, 1, embed_size)\n",
    "    \n",
    "    # Initialize the caption with the start token <SOS>\n",
    "    caption = [vocab.stoi[\"<SOS>\"]]\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Convert current caption sequence to tensor and pass through decoder\n",
    "        caption_tensor = torch.tensor(caption).unsqueeze(0).to(device)  # Shape: (1, seq_len)\n",
    "        \n",
    "        # Predict next word\n",
    "        with torch.no_grad():\n",
    "            outputs = decoder(features, caption_tensor)\n",
    "        \n",
    "        # Get the most likely word index from the decoder's output\n",
    "        predicted_idx = outputs.argmax(2)[:, -1].item()\n",
    "        caption.append(predicted_idx)\n",
    "        \n",
    "        # If the predicted word is <EOS>, stop generating\n",
    "        if predicted_idx == vocab.stoi[\"<EOS>\"]:\n",
    "            break\n",
    "\n",
    "    # Convert word indices to words\n",
    "    caption_words = [vocab.itos[idx] for idx in caption[1:]]  # Exclude <SOS>\n",
    "    return \" \".join(caption_words)\n",
    "\n",
    "# Define transformation for the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# Paths\n",
    "image_path = '/kaggle/input/flickr-8k-images/Flickr8k/Flickr8k_Dataset/Flicker8k_Dataset/1001773457_577c3a7d70.jpg'  # Path to the image you want to caption\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_tensor = load_image(image_path, transform)\n",
    "\n",
    "# Generate the caption\n",
    "caption = generate_caption(encoder, decoder, image_tensor, vocab)\n",
    "print(\"Generated Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1666473,
     "sourceId": 2733544,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
